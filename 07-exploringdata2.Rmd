# Exploring data #2

[Download](https://github.com/geanders/RProgrammingForResearch/raw/master/slides/CourseNotes_Week7.pdf) a pdf of the lecture slides covering this topic.

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(knitr)
library(faraway)
data(worldcup)
library(ggthemes)
```

---

## Simple statistical tests in R


Let's pull the fatal accident data just for the county that includes Las Vegas, NV. 
Each US county has a unique identifier (FIPS code), composed of a two-digit state FIPS and a three-digit county FIPS code. The state FIPS for Nevada is 32; the county FIPS for Clark County is 003. Therefore, we can filter down to Clark County data in the FARS data we collected with the following code:

```{r message = FALSE, error = FALSE}
library(readr)
library(dplyr)
clark_co_accidents <- read_csv("data/accident.csv") %>% 
  filter(STATE == 32 & COUNTY == 3)
```

We can also check the number of accidents: 

```{r}
clark_co_accidents %>% 
  count()
```

We want to test if the probability, on a Friday or Saturday, of a fatal accident occurring is higher than on other days of the week. Let's clean the data up a bit as a start: 

```{r message = FALSE, warning = FALSE}
library(tidyr)
library(lubridate)
clark_co_accidents <- clark_co_accidents %>% 
  select(DAY, MONTH, YEAR) %>% 
  unite(date, DAY, MONTH, YEAR, sep = "-") %>% 
  mutate(date = dmy(date))
```

Here's what the data looks like now: 

```{r}
clark_co_accidents %>% 
  slice(1:5)
```

Next, let's get the count of accidents by date: 

```{r}
clark_co_accidents <- clark_co_accidents %>% 
  group_by(date) %>% 
  count() %>% 
  ungroup()
clark_co_accidents %>% 
  slice(1:3)
```

We're missing the dates without a fatal crash, so let's add those. First, create a dataframe
with all dates in 2016:

```{r}
all_dates <- data_frame(date = seq(ymd("2016-01-01"), 
                                   ymd("2016-12-31"), by = 1))
all_dates %>% 
  slice(1:5)
```

Then merge this with the original dataset on Las Vegas fatal crashes and make any day missing from the fatal crashes dataset have a "0" for number of fatal accidents (`n`):

```{r}
clark_co_accidents <- clark_co_accidents %>% 
  right_join(all_dates, by = "date") %>% 
  # If `n` is missing, set to 0. Otherwise keep value.
  mutate(n = ifelse(is.na(n), 0, n))
clark_co_accidents %>% 
  slice(1:3)
```

Next, let's add some information about day of week and weekend: 

```{r}
clark_co_accidents <- clark_co_accidents %>% 
  mutate(weekday = wday(date, label = TRUE), 
         weekend = weekday %in% c("Fri", "Sat"))
clark_co_accidents %>% 
  slice(1:3)
```

Now let's calculate the probability that a day has at least one fatal crash, separately for weekends and weekdays: 

```{r}
clark_co_accidents <- clark_co_accidents %>% 
  mutate(any_crash = n > 0)
crash_prob <- clark_co_accidents %>% 
  group_by(weekend) %>% 
  summarize(n_days = n(),
            crash_days = sum(any_crash)) %>% 
  mutate(prob_crash_day = crash_days / n_days)
crash_prob
```

In R, you can use `prop.test` to test if two proportions are equal. Inputs include the total number of trials in each group (`n =`) and the number of "successes"" (`x = `):

```{r}
prop.test(x = crash_prob$crash_days, 
          n = crash_prob$n_days)
```

I won't be teaching in this course how to find the correct statistical test. That's 
something you'll hopefully learn in a statistics course. There are also a variety of books that can help you with this, including some that you  can access free online through CSU's library. One servicable introduction is "Statistical Analysis with R for Dummies".

You can create an object from the output of any statistical test in R. Typically, this will be (at least at some level) in an object class called a "list":

```{r}
vegas_test <- prop.test(x = crash_prob$crash_days, 
                        n = crash_prob$n_days)
is.list(vegas_test)
```

So far, we've mostly worked with two object types in R, **dataframes** and **vectors**. In the next subsection we'll look more at two object classes we haven't looked at much,
**matrices** and **lists**. Both have important roles once you start applying more
advanced methods to analyze your data. 


## Matrices

A matrix is like a data frame, but all the values in all columns must be of the same class (e.g., numeric, character). R uses matrices a lot for its underlying math (e.g., for the linear algebra operations required for fitting regression models). R can do matrix operations quite quickly.

You can create a matrix with the `matrix` function. Input a vector with the values to fill the matrix and `ncol` to set the number of columns:

```{r cab}
foo <- matrix(1:10, ncol = 5)
foo
```

By default, the matrix will fill up by column. You can fill it by row with the `byrow` function:

```{r cac}
foo <- matrix(1:10, ncol = 5, byrow = TRUE)
foo
```

In certain situations, you might want to work with a matrix instead of a data frame (for example, in cases where you were concerned about speed -- a matrix is more memory efficient than the corresponding data frame). If you want to convert a data frame to a matrix, you can use the `as.matrix` function:

```{r cad}
foo <- data.frame(col_1 = 1:2, col_2 = 3:4,
                  col_3 = 5:6, col_4 = 7:8,
                  col_5 = 9:10)
(foo <- as.matrix(foo))
```

You can index matrices with square brackets, just like data frames:

```{r cae}
foo[1, 1:2]
```

You cannot, however, use `dplyr` functions with matrices:

```{r caf, eval = FALSE}
foo %>% filter(col_1 == 1)
```

All elements in a matrix must have the same class. 

The matrix will default to make all values the most general class of any of the values, in any column. For example, if we replaced one numeric value with the character "a", everything would turn into a character:

```{r cag}
foo[1, 1] <- "a"
foo
```

---

## Lists

A list has different elements, just like a data frame has different columns. However, the different elements of a list can have different lengths (unlike the columns of a data frame). The different elements can also have different classes.

```{r cah}
bar <- list(some_letters = letters[1:3],
            some_numbers = 1:5, 
            some_logical_values = c(TRUE, FALSE))
bar
```

To index an element from a list, use double square brackets. You can use bracket indexing either with numbers (which element in the list?) or with names:

```{r cai}
bar[[1]]
bar[["some_numbers"]]
```

You can also index lists with the `$` operator:

```{r caii}
bar$some_logical_values
```

To access a specific value within a list element we can index the element e.g.:

```{r caiii}
bar[[1]][[2]]
```

Lists can be used to contain data with an unusual structure and / or lots of different components. For example, the information from fitting a regression is often stored as a list:

```{r caj}
my_mod <- glm(rnorm(10) ~ c(1:10))
is.list(my_mod)
```

The `names` function returns the name of each element in the list:

```{r cak}
head(names(my_mod), 3)
my_mod[["coefficients"]]
```

A list can even contain other lists. We can use the `str` function to see the structure of a list:

```{r cakk}
a_list <- list(list("a", "b"), list(1, 2))

str(a_list)
```

Sometimes you'll see unnecessary lists-of-lists, perhaps when importing data into R created. Or a list with multiple elements that you would like to combine. You can remove a level of hierarchy from a list using the `flatten` function from the `purrr` package:

```{r cakl, warning = FALSE}
library(purrr)
a_list
flatten(a_list)
```

Let's look at the list object from the statistical test we ran for Las Vegas: 

```{r}
str(vegas_test)
```

Using `str` to print out the list's structure doesn't produce the easiest to digest output. We can use the `???` function from the `listviewer` package to create a widget in the `viewer` pane to more esily explore our list.

```{r}
library(listviewer)
jsonedit(vegas_test) # better function?
```


We can pull out an element using the `$` notation: 

```{r}
vegas_test$p.value
```

Or using the `[[` notation:

```{r}
vegas_test[[4]]
```

You may have noticed, though, that this output is not a tidy dataframe. 
Ack! That means we can't use all the tidyverse tricks we've learned so far in the course!
Fortunately, David Robinson noticed this problem and came up with a package called `broom` that can "tidy up" a lot of these kinds of objects.

The `broom` package has three main functions: 

- `glance`: Return a one-row, tidy dataframe from a model or other R object
- `tidy`: Return a tidy dataframe from a model or other R object
- `augment`: "Augment" the dataframe you input to the statistical function

Here is the output for `tidy` for the `vegas_test` object (`augment`
won't work for this type of object, and `glance` gives the same thing as `tidy`): 

```{r}
library(broom)
tidy(vegas_test)
```

---

## Regression models 

### Formula structure

*Regression models* can be used to estimate how the expected value of a *dependent variable* changes as *independent variables* change. \medskip

In R, regression formulas take this structure:

```{r eval = FALSE}
## Generic code
[response variable] ~ [indep. var. 1] +  [indep. var. 2] + ...
```

Notice that a tilde, `~`, is used to separate the independent and dependent variables and that a plus sign, `+`, is used to join independent variables. This format mimics the statistical notation:

$$
Y_i \sim X_1 + X_2 + X_3
$$

You will use this type of structure in R fo a lot of different function calls, including those for linear models (fit with the `lm` function) and generalized linear models (fit with the `glm` function).

There are some conventions that can be used in R formulas. Common ones include:

```{r echo = FALSE}
for_convs <- data.frame(Convention = c("`I()`", "`:`", "`*`", "`.`",
                                       "`1`", "`-`"),
                        Meaning = c("evaluate the formula inside `I()` before fitting (e.g., `I(x1 + x2)`)",
                                    "fit the interaction between `x1` and `x2` variables",
                                    "fit the main effects and interaction for both variables (e.g., `x1*x2` equals `x1 + x2 + x1:x2`)",
                                    "include as independent variables all variables other than the response (e.g., `y ~ .`)",
                                    "intercept (e.g., `y ~ 1` for an intercept-only model)",
                                    "do not include a variable in the data frame as an independent variables (e.g., `y ~ . - x1`); usually used in conjunction with `.` or `1`"))
pander::pander(for_convs, split.cells = c(1,1,58),
               justify = c("center", "left"))
```

### Linear models

To fit a linear model, you can use the function `lm()`. This function is part of the `stats` package, which comes installed with base R. In this function, you can use the `data` option to specify the data frame from which to get the vectors. 

```{r, echo=FALSE}
library(forcats)
nepali_fct <- as_data_frame(nepali) %>%
              mutate(sex = fct_recode(factor(sex), Male = "1", Female = "2"))
```

```{r}
mod_a <- lm(wt ~ ht, data = nepali)
```

This previous call fits the model:

$$ Y_{i} = \beta_{0} + \beta_{1}X_{1,i} + \epsilon_{i} $$

where:

- $Y_{i}$ : weight of child $i$
- $X_{1,i}$ : height of child $i$


If you run the `lm` function without saving it as an object, R will fit the regression and print out the function call and the estimated model coefficients:

```{r}
lm(wt ~ ht, data = nepali)
```

However, to be able to use the model later for things like predictions and model assessments, you should save the output of the function as an R object:

```{r}
mod_a <- lm(wt ~ ht, data = nepali)
```

This object has a special class, `lm`:

```{r}
class(mod_a)
```

This class is a special type of list object. If you use `is.list` to check, you can confirm that this object is a list:

```{r}
is.list(mod_a)
```

There are a number of functions that you can apply to an `lm` object. These include:

```{r echo = FALSE}
mod_objects <- data.frame(Function = c("`summary`", "`coefficients`", 
                                   "`fitted`",
                                   "`plot`", "`residuals`"),
                          Description = c("Get a variety of information on the model, including coefficients and p-values for the coefficients",
                                   "Pull out just the coefficients for a model",
                                   "Get the fitted values from the model (for the data used to fit the model)",
                                   "Create plots to help assess model assumptions",
                                   "Get the model residuals"))
pander::pander(mod_objects, split.cells = c(1,1,58),
               justify = c("center", "left"))
```

For example, you can get the coefficients from the model by running:

```{r}
coefficients(mod_a)
```

The estimated coefficient for the intercept is always given under the name "(Intercept)". Estimated coefficients for independent variables are given based on their column names in the original data ("ht" here, for $\beta_1$, or the estimated increase in expected weight for a one unit increase in height).

You can use the output from a `coefficients` call to plot a regression line based on the model fit on top of points showing the original data (Figure \@ref(fig:modelcoefplot)). 

```{r modelcoefplot, fig.height = 3.5, fig.width = 5, warning = FALSE, fig.align = "center", fig.cap = "Example of using the output from a coefficients call to add a regression line to a scatterplot."}
mod_coef <- coefficients(mod_a)
ggplot(nepali, aes(x = ht, y = wt)) + 
  geom_point(size = 0.2) + 
  xlab("Height (cm)") + ylab("Weight (kg)") + 
  geom_abline(aes(intercept = mod_coef[1],
                  slope = mod_coef[2]), col = "blue")
```

```{block, type = "rmdnote"}
You can also add a linear regression line to a scatterplot by adding the geom `geom_smooth` using the argument `method = "lm"`.
```

You can use the function `residuals` on an `lm` object to pull out the residuals from the model fit:

```{r}
head(residuals(mod_a))
```

The result of a `residuals` call is a vector with one element for each of the non-missing observations (rows) in the data frame you used to fit the model. Each value gives the different between the model fitted value and the observed value for each of these observations, in the same order the observations show up in the data frame. The residuals are in the same order as the observations in the original data frame.

```{block, type = "rmdtip"}
You can also use the shorter function `coef` as an alternative to `coefficients` and the shorter function `resid` as an alternative to `residuals`.
```

As noted in the subsection on simple statistics functions, the `summary` function returns different output depending on the type of object that is input to the function. If you input a regression model object to `summary`, the function gives you a lot of information about the model. For example, here is the output returned by running `summary` for the linear regression model object we just created:

```{r}
summary(mod_a)
```

This output includes a lot of useful elements, including (1) basic summary statistics for the residuals (to meet model assumptions, the median should be around zero and the absolute values fairly similar for the first and third quantiles), (2) coefficient estimates, standard errors, and p-values, and (3) some model summary statistics, including residual standard error, degrees of freedom, number of missing observations, and F-statistic.

The object returned by the `summary()` function when it is applied to an `lm` object is a list, which you can confirm using the `is.list` function:

```{r}
is.list(summary(mod_a))
```

With any list, you can use the `names` function to get the names of all of the different elements of the object:

```{r}
names(summary(mod_a))
```

You can use the `$` operator to pull out any element of the list. For example, to pull out the table with information on the estimated model coefficients, you can run:

```{r}
summary(mod_a)$coefficients
```

The `plot` function, like the `summary` function, will give different output depending on the class of the object that you input. For an `lm` object, you can use the `plot` function to get a number of useful diagnostic plots that will help you check regression assumptions (Figure \@ref(fig:plotlmexample)):

```{r eval = FALSE}
plot(mod_a)
```

```{r plotlmexample, echo = FALSE, out.width = '\\textwidth', fig.align = "center", fig.cap = "Example output from running the plot function with an lm object as the input."}
oldpar <- par(mfrow = c(2, 2))
plot(mod_a)
par(oldpar)
```

You can also use binary variables or factors as independent variables in regression models. For example, in the `nepali` dataset, `sex` is a factor variable with the levels "Male" and "Female". You can fit a linear model of weight regressed on sex for this data with the call:

```{r}
mod_b <- lm(wt ~ sex, data = nepali)
```

This call fits the model:

$$ Y_{i} = \beta_{0} + \beta_{1}X_{1,i} + \epsilon_{i} $$

where $X_{1,i}$ : sex of child $i$, where 0 = male and 1 = female. 

Here are the estimated coefficients from fitting this model:

```{r}
summary(mod_b)$coefficients
```

You'll notice that, in addition to an estimated intercept (`(Intercept)`), the other estimated coefficient is `sexFemale` rather than just `sex`, although the column name in the data frame input to `lm` for this variable is `sex`. 

This is because, when a factor or binary variable is input as an independent variable in a linear regression model, R will fit an estimated coefficient for all levels of factors *except* the first factor level. By default, this first factor level is used as the baseline level, and so its estimated mean is given by the estimated intercept, while the other model coefficients give the estimated *difference* from this baseline. 

For example, the model fit above tells us that the estimated mean weight of males is `r round(coef(mod_b)[1], 1)`, while the estimated mean weight of females is `r round(coef(mod_b)[1], 1)` + `r round(coef(mod_b)[2], 1)` = `r round(coef(mod_b)[1], 1) + round(coef(mod_b)[2], 1)`.

<!-- If you would prefer that a different level of the factor be the baseline (for example, "Female" rather than "Male" for the previous regression), you can do that by using the `levels` argument in the `factor` function to reset factor levels. For example: -->

<!-- ```{r} -->
<!-- nepali_reset <- nepali %>% -->
<!--   mutate(sex = factor(sex, levels = c("2", "1"))) -->

<!-- mod_b_reset <- lm(wt ~ sex, data = nepali_reset) -->
<!-- summary(mod_b_reset)$coef -->
<!-- ``` -->

<!-- Now, `(Intercept)` gives the estimated mean weight for females, while the second estimated coefficient gives the estimated mean difference for males compared to the expected value for females. -->

---

### Generalized linear models (GLMs)

You can fit a variety of models, including linear models, logistic models, and Poisson models, using generalized linear models (GLMs). \medskip

For linear models, the only difference between `lm` and `glm` are the mechanics of how they estimate the model coefficients (`lm` uses least squares while `glm` uses maximum likelihood). You will (almost always) get exactly the same estimated coefficients regardless of whether you use `glm` or `lm` to fit a linear regression. 

For example, here is the code to fit a linear regression model for weight regressed on height from the `nepali` dataset:

```{r}
mod_c <- glm(wt ~ ht, data = nepali)
```

This call fits the same regression model I fit earlier with the `lm` function and saved as `mod_a`. You can see that the two methods give exactly the same coefficient estimates:

```{r}
coef(mod_c)
coef(mod_a)
```

Unlike the `lm` function, however, the `glm` function also allows you to fit other model types, including logistic and Poisson models. You can specify the model type using the `family` argument to the `glm` call:

```{r echo = FALSE}
glm_types <- data.frame(type = c("Linear", "Logistic", "Poisson"),
                        opt = c("`family = gaussian(link = 'identity')`",
                                "`family = binomial(link = 'logit')`", 
                                "`family = poisson(link = 'log')`"))
knitr::kable(glm_types, col.names = c("Model type", "`family` argument"))
```

For example, say we wanted to fit a logistic regression for the `nepali` data of whether the probability of a child weighing more than 13 kg is associated with the child's sex. 

First, create a binary variable in the `nepali` dataset, `wt_over_13`, that is `TRUE` if a child weighed more than 13 kilograms and FALSE otherwise. You can use the `mutate` function from `dplyr` to add this new column (which, as a note, is a logical vector):

```{r}
nepali <- nepali %>% 
  mutate(wt_over_13 = wt > 13)
head(nepali)
```

Now you can fit a logistic regression of `wt_over_13` regressed on sex, using a logistic model:

```{r}
mod_d <- glm(wt_over_13 ~ sex, data = nepali,
             family = binomial(link = "logit"))
```

Elements of a GLM can be pulled out in the same way that we looked at elements from the linear model fit with `lm`. For example, to see a table of estimated model coefficients, you can run:

```{r}
summary(mod_d)$coef
```

Because this model was a logistic model, fit with a log link, here the model coefficient estimate for `sexFemale` gives an estimate of the **log odds** of weight higher than 13 kg associated with females versus males. The p-value for this estimate (`Pr(>|z|)` = `r round(summary(mod_d)$coef[2, 4], 2)`) isn't very small, suggesting that the difference between male and female children in the odds of weighing more than 13 kg is not statistically significant. 

---

## Handling model objects

The `broom` package contains tools for converting statistical objects into nice tidy data frames. The tools in the `broom` package make it easy to process statistical results in R using the tools of the `tidyverse`.

### broom::tidy

The `tidy()` function returns a data frame with information on the fitted model terms. For example, when applied to one of our linear models we get:

```{r tidy_example}
  library(broom)
  kable(tidy(mod_a), digits = 3)
```

```{r tidy_class}
  class(tidy(mod_a))
```

You can pass arguments to the `tidy()` function. For example, include confidence intervals:

```{r tidy_cis}
  kable(tidy(mod_a, conf.int = TRUE), digits = 3)
```

---

### broom::augment

The `augment()` function adds information about a fitted model to the dataset used to fit the model. For example, when applied to one of our linear models we get information on the fitted values and residuals included in the output:

```{r augment_example, warning = FALSE}
  kable(head(broom::augment(mod_a), 3), digits = 3)
```

---

### broom::glance

The `glance()` functions returns a one row summary of a fitted model object: For example:

```{r glance_example, warning = FALSE}
  kable(glance(mod_a, conf.int = TRUE), digits = 3)
```

---

### References-- statistics in R

One great (and free online for CSU students through our library) book to find out more about using R for basic statistics is:

- [Introductory Statistics with R](http://discovery.library.colostate.edu/Record/.b44705323)

If you want all the details about fitting linear models and GLMs in R, Julian Faraway's books are fantastic. He has one on linear models and one on extensions including logistic and Poisson models:

- [Linear Models with R](http://discovery.library.colostate.edu/Record/.b41119691) (also free online through the CSU library)
- [Extending the Linear Model with R](http://www.amazon.com/Extending-Linear-Model-Generalized-Nonparametric/dp/158488424X/ref=sr_1_1?ie=UTF8&qid=1442252668&sr=8-1&keywords=extending+linear+model+r)

---

## In-course exercise

### Running a simple statistical test

In last week's in-course exercise, we found out that about 17% of babies born in the 
United States between 1980 and 1995 had names that started with an "A" or "K" (60,893
babies out of 359,011). 

- What is the proportion of people with names that start with an "A" or "K" in our class?
- Use a simple statistical test to test the hypothesis that the class comes from a 
binomial distribution with the same distribution as babies born in the US between 1980 and 
1995, in terms of chance of having a name that starts with "A" or "K".
- You may get the warning "Chi-squared approximation may be incorrect". See if you can 
figure out this warning.
- If you are able to figure out and run a statistical test, go back to your notes from 
last week and write a full script, starting from loading the `babynames` data and creating
your own vector of the names in this class, all the way through to running the statistical 
test you picked. 

---

#### Example R code

Here is a vector with names in our class: 

```{r}
library(stringr)
student_list <- data_frame(name = c("Aeriel", "Rebecca", "Grant",
                                    "Amy", "Jessy", "Alyssa",
                                    "Camron", "Anastasia", "Kyle",
                                    "Ana", "Amanda", "Kathleen", 
                                    "Kyle", "Ana", "Amanda", "Kathleen",
                                    "Kayla", "Nichole", "Randy", "Katy",
                                    "Devin"))
student_list <- student_list %>% 
  mutate(first_letter = str_sub(name, 1, 1))
student_list %>% 
  slice(1:3)
```

Let's get the total number of students, and then the total number with a name that 
starts with "A" or "K": 

```{r}
tot_students <- student_list %>% 
  count()
tot_students

a_or_k_students <- student_list %>% 
  mutate(a_or_k = first_letter %in% c("A", "K")) %>% 
  group_by(a_or_k) %>% 
  count()
a_or_k_students
```

The proportion of students with names starting with "A" or "K" are `r a_or_k_students$n[2]` / 
`r tot_students$n[1]` = `r round(a_or_k_students$n[2] / tot_students$n[1], 2)`.

You could run a statistical test comparing these two proportions:

```{r}
prop.test(x = c(60893, 14), n = c(359011, 21))
```

You could also test whether the proportion in our class is consistent with the null
hypothesis that you were drawn from a binomial distribution with a proportion of 0.17
(in-line with the national values):

```{r}
prop.test(x = 14, n = 21, p = 0.17)
```

Finally, when we run this test, we get the warning that "Chi-squared approximation may be incorrect". Based on googling 'r prop.test "Chi-squared approximation may be incorrect"', 
it sounds like we might be getting this error because we have a pretty low number of 
people in the class. One recommendation is to instead run a Chi-square contingency table test, which you can do with `chisq.test`, while setting `simulate.p.values` to TRUE. This requires
a slightly different set-up for the data (which you can figure out by trying out the
examples in the helpfile for `chisq.test`):

```{r}
a_or_k_names <- matrix(c(14, 60893, 7, 298118), 
                       nrow = 2)
a_or_k_names
chisq.test(a_or_k_names, simulate.p.value = TRUE) 
```

You could also use `binom.test`, which will run as an exact binomial test:

```{r}
binom.test(x = 14, n = 21, p = 0.17)
```

### Using regression models to explore data #1

For this exercise, you will need the following packages. If do not have them already, you will need to install them. 

```{r}
library(ggplot2)
library(broom)
library(ggfortify)
```

For this part of the exercise, you'll use a dataset on weather, air pollution, and mortality counts in Chicago, IL. This dataset is called `chicagoNMMAPS` and is part of the `dlnm` package. Change the name of the data frame to `chic` (this object name is shorter and will be easier to work with). Check out the data a bit to see what variables you have, and then perform the following tasks:

- Write out (on paper, not in R) the regression equation for regressing dewpoint temperature on temperature. 
- Try fitting a linear regression of dew point temperature (`dptp`) regressed on temperature (`temp`). Save this model as the object `mod_1` (i.e., is the dependent variable of 
dewpoint temperature linearly associated with the independent variable of temperature). 
- Based on this regression, does there seem to be a relationship between temperature and dewpoint temperature in Chicago? (Hint: Try using `glance` and `tidy` from the `broom` package on the model object to get more information about the model you fit.) What is the coefficient for temperature (in other words, for every 1 degree increase in temperature, how much do we expect the dewpoint temperature to change?)? What is the p-value for the coefficient for temperature?
- Plot temperature (x-axis) versus dewpoint temperature (y-axis) for Chicago. Add in the regression line from the model you fit by using the results from `augment`.
- Use `autoplot` on the model object to generate some model diagnostic plots (make sure you have the `ggfortify` package loaded and installed).
- Try fitting the regression as a GLM, using `glm()` (but still assuming the outcome variable is normally distributed). Are your coefficients different?

---

#### Example R code:

The regression equation for the model you want to fit, regressing dewpoint temperature on temperature, is:

$$
Y_t \sim \beta_0 + \beta_1 X_t
$$

where $Y_t$ is the dewpoint temperature on day $t$, $X_t$ is the temperature on day $t$, and $\beta_0$ and $\beta_1$ are model coefficients. 

Install and load the `dlnm` package and then load the `chicagoNMMAPS` data. Change the name of the data frame to `chic`, so it will be shorter to call for the rest of your work. 
```{r, message = FALSE, warning = FALSE}
# install.packages("dlnm")
library(dlnm)
data("chicagoNMMAPS")
chic <- chicagoNMMAPS
```

Fit a linear regression of `dptp` on `temp` and save as the object `mod_1`:

```{r}
mod_1 <- lm(dptp ~ temp, data = chic)
mod_1
```

Use functions from the `broom` package to pull the same information about the model in a "tidy" format. To find out if the evidence for a linear association between temperature and dewpoint temperature, use the `tidy` function to get model coefficients in a tidy format:

```{r}
tidy(mod_1)
```

There does seem to be an association between temperature and dewpoint temperature: a unit increase in temperature is associated with a `r round(coef(mod_1)[2], 1)` unit increase in dewpoint temperature. The p-value for the temperature coefficient is <2e-16. This is far below 0.05, which suggests we would be very unlikely to see such a strong association by chance if the null hypothesis, that the two variables are not associated, were true.

You can also check overall model summaries using the `glance` function:

```{r}
glance(mod_1)
```

To create plots of the observations and the fit model, use the `augment` function to add model output (e.g., predictions, residuals) to the original data frame of observed temperatures and dew point temperatures:

```{r warning = FALSE, message = FALSE}
augment(mod_1) %>% 
  slice(1:3)
```

Plot these two variables and add in the fitted line from the model (note: I've used the `color` option to make the color of the points gray). Use the output from `augment` to create a plot of the original data, with the predicted values used to plot a fitted line. 

```{r warning = FALSE, message = FALSE, fig.width = 5, fig.height = 4}
augment(mod_1) %>% 
  ggplot(aes(x = temp, y = dptp)) + 
  geom_point(size = 0.8, alpha = 0.5, col = "gray") + 
  geom_line(aes(x = temp, y = .fitted), color = "red", size = 2) + 
  theme_classic()
```

Plot some plots to check model assumptions for the model you fit using the `autoplot` function on your model object:

```{r, fig.width = 6, fig.height = 6}
autoplot(mod_1)
```

Try fitting the model using `glm()`. Call it `mod_1a`. Compare the coefficients for the two models. You can use the `tidy` function on an `lm` or `glm` object to pull out just the model coefficients and associated model results. Here, I've used a pipeline of code to create a tidy data frame that merges these "tidy" coefficient outputs (from the two models) into a single data frame):

```{r}
mod_1a <- glm(dptp ~ temp, data = chic)

tidy(mod_1) %>% 
  select(term, estimate) %>% 
  inner_join(mod_1a %>% tidy() %>% select(term, estimate), by = "term") %>% 
  rename(estimate_lm_mod = estimate.x,
         estimate_glm_mod = estimate.y)
```

The results from the two models are identical.

As a note, you could have also just run `tidy` on each model object, without merging them together into a single data frame:

```{r}
tidy(mod_1)
tidy(mod_1a)
```

---

### Using regression models to explore data #2

- Does $PM_{10}$ vary by day of the week? (Hint: The `dow` variable is a factor that gives day of the week. You can do an ANOVA analysis by fitting a linear model using this variable as the independent variable. Some of the overall model summaries will compare this model to an intercept-only model.) What day of the week is PM10 generally highest? (Check the model coefficients to figure this out.) Try to write out (on paper) the regression equation for the model you're fitting.
- Try using `glm()` to run a Poisson regression of respiratory deaths (`resp`) on temperature during summer days. Start by creating a subset with just summer days called `summer`. (Hint: Use the `month` function with the argument `label = TRUE` from `lubridate` to do this-- just pull out the subset where the month is 6, 7, or 8, for "Jun", "Jul", and "Aug".) Try to write out the regression equation for the model you're fitting.
- The coefficient for the temperature variable in this model is our best estimate (based on this model) of the **log relative risk** for a one degree Celcius increase in temperature. What is the **relative risk** associated with a one degree Celsius increase?

---

#### Example R code:

Fit a model of $PM_{10}$ regressed on day of week, where day of week is a factor. 

```{r}
mod_2 <- lm(pm10 ~ dow, data = chic)
tidy(mod_2)
```

Use `glance` to check some of the overall summaries of this model. The `statistic` column here is the F statistic from test comparing this model to an intercept-only model.

```{r}
glance(mod_2)
```

As a note, you may have heard in previous statistics classes that you can use the `anova()` command to compare this model to a model with only an intercept (i.e., one that only fits a global mean and uses that as the expected value for all of the observations). Note that, in this case, the F value from `anova` for this model comparison is the same as the `statistic` you got in the overall summary statistics you get with `glance` in the previous code. 

```{r}
anova(mod_2)
```

The overall p-value from `anova` for with day-of-week coefficients versus the model that just has an intercept is < 2.2e-16. This is well below 0.05, which suggests that day-of-week is associated with PM10 concentration, as a model that includes day-of-week does a much better job of explaining variation in PM10 than a model without it does. 

Use a boxplot to visually compare PM10 by day of week. 

```{r, fig.height = 3, fig.width = 6, warning = FALSE}
ggplot(chic, aes(x = dow, y = pm10)) + 
  geom_boxplot()
```

Now try the same plot, but try using the `ylim = ` option to change the limits on the y-axis for the graph, so you can get a better idea of the pattern by day of week (some of the extreme values are very high, which makes it hard to compare by eye when the y-axis extends to include them all).

```{r, fig.height = 3, fig.width = 6, message = FALSE}
ggplot(chic, aes(x = dow, y = pm10)) + 
  geom_boxplot() + 
  ylim(c(0, 100))
```

Create a subset called `summer` with just the summer days:

```{r}
library(lubridate)
summer <- chic %>%
  mutate(month = month(date, label = TRUE)) %>% 
  filter(month %in% c("Jun", "Jul", "Aug"))
summer %>% 
  slice(1:3)
```

Use `glm()` to fit a Poisson model of respiratory deaths regressed on temperature. Since you want to fit a Poisson model, use the option `family = poisson(link = "log")`. 

```{r}
mod_3 <- glm(resp ~ temp, data = summer,
             family = poisson(link = "log"))
glance(mod_3)
tidy(mod_3)
```

Use the fitted model coefficient to determine the relative risk for a one degree Celsius increase in temperature. First, remember that you can use the `tidy()` function to read out the model coefficients. The second of these is the value for the temperature coefficient. That means that you can use indexing (`[2]`) to get just that value. That's the log relative risk; take the exponent to get the relative risk.

```{r}
tidy(mod_3) %>% 
  filter(term == "temp") %>% 
  mutate(log_rr = exp(estimate))
```

As a note, you can use the `conf.int` parameter in `tidy` to also pull confidence intervals:

```{r}
tidy(mod_3, conf.int = TRUE)
```

You could use this to get the confidence interval for relative risk (check out the `mutate_at` function if you haven't seen it before):

```{r}
tidy(mod_3, conf.int = TRUE) %>% 
  select(term, estimate, conf.low, conf.high) %>% 
  filter(term == "temp") %>% 
  mutate_at(vars(estimate:conf.high), funs(exp(.)))
```

---

### Exploring Fatality Analysis Reporting System (FARS) data

- Visit http://metrocosm.com/10-years-of-traffic-accidents-mapped.html and explore the interactive visualization created by Max Galka using this public dataset on US fatal motor vehicle accidents.
- Go to [FARS web page](http://www.nhtsa.gov/FARS). We want to get the raw data on fatal accidents. Navigate this page to figure out how you can get this raw data for the whole county for 2015 (hint: you'll need to access the raw data using FTP, and you may have more success with some web browsers that others). Save 2015 "National" data (csv format) to your computer. What is the structure of how this data is saved (e.g., directory structure, file structure)?
- On the [FARS web page](http://www.nhtsa.gov/FARS), find the documentation describing this raw data. (The relevant documentation file is called *Fatality Analysis Reporting System (FARS) Analytical User's Manual 1975-2015*) Look through both this documentation and the raw files you downloaded to figure out what information is included in the data. 
- Read the `accident.csv` file for 2015 into R (this is one of the files you'll get if you download the raw data for 2015). Use the documentation to figure out what each column represents. 
- Discuss what steps you would need to take to create the following plot. To start, don't write any code, just develop a plan. Talk about what the dataset should look like right before you create the plot and what functions you could use to get the data from its current format to that format. (Hint: Functions from the `lubridate` package will be very helpful, including `yday` and `wday`).
- Discuss which of the variables in this dataset could be used to merge the dataset with other appropriate data, either other datasets in the FARS raw data, or outside datasets. 
- Try to write the code to create the plot below. This will include some code for cleaning the data and some code for plotting. There is an example answer below, but I'd like you to try to figure it out yourselves first.

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.width = 6, fig.height = 3, fig.align = "center"}
library(tidyverse)
library(lubridate)
library(ggthemes)

accident <- read_csv("data/accident.csv") %>%
  select(DAY:MINUTE) %>%
  select(-DAY_WEEK) %>%
  unite(date, DAY:MINUTE, sep = "-", remove = FALSE) %>%
  mutate(date = dmy_hm(date), 
         yday = yday(date),
         weekday = wday(date, label = TRUE, abbr = FALSE),
         weekend = weekday %in% c("Saturday", "Sunday")) 

accident %>% 
  filter(!is.na(yday)) %>%
  group_by(yday) %>%
  summarize(accidents = n(),
            weekend = first(weekend)) %>%
  ggplot(aes(x = yday, y = accidents, color = weekend)) + 
  geom_point(alpha = 0.5) + 
  xlab("Day of the year in 2015") + 
  ylab("# of fatal accidents") + 
  theme_few() + 
  geom_smooth(se = FALSE)
```

---

#### Example R code

Here is example code for the section above:

```{r check12, warning = FALSE, message = FALSE, fig.width = 6, fig.height = 3, fig.align = "center"}
library(tidyverse)
library(lubridate)
library(ggthemes)

accident <- read_csv("data/accident.csv") %>%
  select(DAY:MINUTE) %>%
  select(-DAY_WEEK) %>%
  unite(date, DAY:MINUTE, sep = "-", remove = FALSE) %>%
  mutate(date = dmy_hm(date), 
         yday = yday(date),
         weekday = wday(date, label = TRUE, abbr = FALSE),
         weekend = weekday %in% c("Saturday", "Sunday")) 

accident %>% 
  filter(!is.na(yday)) %>%
  group_by(yday) %>%
  summarize(accidents = n(),
            weekend = first(weekend)) %>%
  ggplot(aes(x = yday, y = accidents, color = weekend)) + 
  geom_point(alpha = 0.5) + 
  xlab("Day of the year in 2015") + 
  ylab("# of fatal accidents") + 
  theme_few() + 
  geom_smooth(se = FALSE)
```

---

<!-- ### Using a function and `purrr` to create state-specific plots -->

<!-- Next, you will write a function to create state-specific plots from this data, then use it to create plots for the states of Colorado, Texas, California, and New York.  -->

<!-- - The FARS data includes a column called `STATE`, but it gives state as a one- or two-digit code, rather than by name. These codes are the state Federal Information Processing Standard (FIPS) codes. A dataset with state names and FIPS codes is available at http://www2.census.gov/geo/docs/reference/state.txt. Read that data into an R object called `state_fips` and clean it so the first few lines look like this (hint: to change the `state` column to an integer class, you can use the function `as.integer`): -->

<!-- ```{r check11, echo = FALSE, warning = FALSE, message = FALSE} -->
<!-- state_fips <- read_delim("http://www2.census.gov/geo/docs/reference/state.txt", -->
<!--                          delim = "|") %>% -->
<!--   rename(state = STATE, -->
<!--          state_name = STATE_NAME) %>% -->
<!--   select(state, state_name) %>% -->
<!--   mutate(state = as.integer(state)) -->
<!-- state_fips %>% slice(1:5) -->
<!-- ``` -->

<!-- - Read the 2015 FARS data into an R object named `accident`. Use all the date and time information to create a column named `date` with the date and time of the accident. Include information on whether the accident was related to drunk driving (FALSE if there were 0 drunk drivers, TRUE if there were one or more), and create columns that gives whether the accident was during the day (7 AM to 7 PM) or not as well as the month of the accident (for this last column, you can either retain it from the original data or recalculate it based on the new `date` variable). Filter out any values where the date-time does not render (i.e., `date` is a missing value). The first few rows of the cleaned data frame should look like this: -->

<!-- ```{r check10, warning = FALSE, message = FALSE, echo = FALSE} -->
<!-- accident <- read_csv("data/accident.csv") %>% -->
<!--   select(STATE, DAY:MINUTE, DRUNK_DR) %>% -->
<!--   rename(state = STATE,  -->
<!--          drunk_dr = DRUNK_DR) %>% -->
<!--   select(-DAY_WEEK) %>% -->
<!--   unite(date, DAY:MINUTE, sep = "-") %>% -->
<!--   mutate(date = dmy_hm(date),  -->
<!--          drunk_dr = drunk_dr >= 1, -->
<!--          daytime = hour(date) %in% c(7:19), -->
<!--          month = month(date)) %>% -->
<!--   filter(!is.na(date)) -->
<!-- accident %>% slice(1:5) -->
<!-- ``` -->

<!-- - Join the information from `state_fips` into the `accident` data frame. There may be a few locations in the `state_fips` data frame that are not included in the `accident` data frame (e.g., Virgin Islands), so when you join keep all observations in `accident` but only the observations in `state_fips` that match at least one row of `accident`. The first few rows of the joined dataset should look like this: -->

<!-- ```{r echo = FALSE} -->
<!-- accident <- accident %>% -->
<!--   left_join(state_fips, by = "state") -->
<!-- accident %>% slice(1:5) -->
<!-- ``` -->

<!-- - Summarize the data to get the total number of accidents in Colorado in each month, separated by (1) daytime and nighttime and (2) related or unrelated to drunk driving  (in other words, in January, how many daytime accidents were there that were unrelated to drunk driving? How many nighttime accidents that were unrelated to drunk driving? etc.). The summarized data should look like this: -->

<!-- ```{r echo = FALSE} -->
<!-- accident %>% -->
<!--   filter(state_name == "Colorado") %>% -->
<!--   group_by(daytime, month, drunk_dr) %>% -->
<!--   summarize(accidents = n()) -->
<!-- ``` -->

<!-- - Write a function that inputs a data frame (`df`) and outputs this type of summary data frame (like the one just created for Colorado) for whatever data is in the input data frame. Below are some examples of how this function should work: -->

<!-- ```{r echo = FALSE} -->
<!-- summarize_fars <- function(df){ -->
<!--   df %>% -->
<!--     group_by(daytime, month, drunk_dr) %>% -->
<!--     summarize(accidents = n()) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- colorado_data <- accident %>%  -->
<!--   filter(state_name == "Colorado") -->

<!-- colorado_summary <- summarize_fars(df = colorado_data) -->
<!-- head(colorado_summary) -->

<!-- # Note also that you can pipe with the new function: -->
<!-- accident %>%  -->
<!--   filter(state_name == "Texas") %>%  -->
<!--   summarize_fars() %>%  -->
<!--   tbl_df() %>%  -->
<!--   slice(1:3) -->
<!-- ``` -->

<!-- - Once you've written the function, see if you can figure out what the following code does. How does the new function fit in? (Note: We could have achieved the same thing with basic `dplyr` code, but this framework will allow you to ultimately do a lot more than you can with `dplyr`.) -->

<!-- ```{r eval = FALSE} -->
<!-- library(purrr) -->

<!-- accident %>%  -->
<!--   filter(state_name %in% c("Colorado", "Texas", "California", "New York")) %>%  -->
<!--   group_by(state_name) %>% -->
<!--   nest() %>%  -->
<!--   mutate(summary = map(data, summarize_fars)) %>%  -->
<!--   select(-data) %>%  -->
<!--   unnest()  -->
<!-- ``` -->

<!-- - Write code to create boxplots for Colorado of the distribution of total accidents within each month. Create separate boxplots for daytime and nighttime accidents, and facet by whether the accident was related to drunk driving. The plot should look like the plot below.  -->

<!-- ```{r check9, echo = FALSE, fig.width = 6, fig.height = 3, fig.align = "center"} -->
<!-- accident %>% -->
<!--   filter(state_name == "Colorado") %>% -->
<!--   mutate(drunk_dr = factor(drunk_dr, labels = c("Unrelated to\ndrunk driving", -->
<!--                                                 "Related to\ndrunk driving")), -->
<!--          daytime = factor(daytime, labels = c("Nighttime", "Daytime"))) %>% -->
<!--   group_by(daytime, month, drunk_dr) %>% -->
<!--   summarize(accidents = n()) %>% -->
<!--   ggplot(aes(x = daytime, y = accidents, group = daytime)) +  -->
<!--   geom_boxplot() +  -->
<!--   facet_wrap(~ drunk_dr, ncol = 2) + -->
<!--   xlab("Time of day") + ylab("# of monthly accidents") +  -->
<!--   ggtitle(paste("Fatal accidents in", "Colorado", "in 2015")) -->
<!-- ``` -->

<!-- - Now write a function called `plot_fars` to create a plot like the one you just made for Colorado for any data frame with the format of `accident` (i.e., same number, types, and names of columns). Test it on subsets of the data for several states (Colorado, Texas, California, and New York). (*Hint*: To get a function to print out a plot created with ggplot, you will need to explicitly print the output from your function. See the examples of using the function below.)  -->

<!-- ```{r check8, echo = FALSE, fig.width = 6, fig.height = 3, fig.align = "center"} -->
<!-- plot_fars <- function(df){ -->
<!--   fars_plot <- df %>% -->
<!--     mutate(drunk_dr = factor(drunk_dr, labels = c("Unrelated to\ndrunk driving", -->
<!--                                                   "Related to\ndrunk driving")), -->
<!--            daytime = factor(daytime, labels = c("Nighttime", "Daytime"))) %>% -->
<!--     group_by(daytime, month, drunk_dr) %>% -->
<!--     summarize(accidents = n()) %>% -->
<!--     ggplot(aes(x = daytime, y = accidents, group = daytime)) + -->
<!--     geom_boxplot() + -->
<!--     facet_wrap(~ drunk_dr, ncol = 2) + -->
<!--     xlab("Time of day") + ylab("# of monthly accidents")  -->
<!-- } -->
<!-- ``` -->

<!-- Here are some examples of what should happen when you run this function: -->

<!-- ```{r, fig.width = 6, fig.height = 3, fig.align = "center"} -->
<!-- co_plot <- plot_fars(df = filter(accident, state_name == "Colorado")) -->
<!-- print(co_plot) -->

<!-- accident %>%  -->
<!--   filter(state_name == "Texas") %>%  -->
<!--   plot_fars() %>%  -->
<!--   print() -->
<!-- ``` -->

<!-- - Once you have written this function, what happens when you run the following code? -->

<!-- ```{r eval = FALSE, fig.width = 6, fig.height = 3, fig.align = "center"} -->
<!-- library(purrr) -->

<!-- state_plots <- accident %>%  -->
<!--   filter(state_name %in% c("Colorado", "Texas", "California", "New York")) %>%  -->
<!--   group_by(state_name) %>% -->
<!--   nest() %>%  -->
<!--   mutate(plots = map(data, plot_fars))  -->

<!-- class(state_plots[["plots"]]) -->
<!-- class(state_plots[["plots"]][[1]]) -->
<!-- print(state_plots[["plots"]][[1]])$plot -->
<!-- ``` -->

<!-- - Install the `cowplot` package (this is a `ggplot2` extension) and then try running the following code. What happens when you run this code? -->

<!-- ```{r eval = FALSE} -->
<!-- plot_grid(plotlist = state_plots[["plots"]],  -->
<!--           ncol = 2, labels = "AUTO") -->
<!-- ``` -->

<!-- #### Example R code -->

<!-- Here is the code to read the dataset with state names and FIPS codes at http://www2.census.gov/geo/docs/reference/state.txt into an R object called `state_fips` and clean it so the first few lines: -->

<!-- ```{r check7, echo = FALSE, warning = FALSE, message = FALSE} -->
<!-- state_fips <- read_delim("http://www2.census.gov/geo/docs/reference/state.txt", -->
<!--                          delim = "|") %>% -->
<!--   rename(state = STATE, -->
<!--          state_name = STATE_NAME) %>% -->
<!--   select(state, state_name) %>% -->
<!--   mutate(state = as.integer(state)) -->
<!-- state_fips %>% slice(1:5) -->
<!-- ``` -->

<!-- Note that you can read this file directly from the website using `read_delim`.  -->

<!-- Read the 2015 FARS data into an R object named `accident`. Use all the date and time information to create a column named `date` with the date and time of the accident. Include information on whether the accident was related to drunk driving (FALSE if there were 0 drunk drivers, TRUE if there were one or more), and create columns that gives whether the accident was during the day (7 AM to 7 PM) or not as well as the month of the accident (for this last column, you can either retain it from the original data or recalculate it based on the new `date` variable). Filter out any values where the date-time does not render (i.e., `date` is a missing value). You can use the following code to do all this: -->

<!-- ```{r check6, warning = FALSE, message = FALSE, echo = FALSE} -->
<!-- accident <- read_csv("data/accident.csv") %>% -->
<!--   select(STATE, DAY:MINUTE, DRUNK_DR) %>% -->
<!--   rename(state = STATE,  -->
<!--          drunk_dr = DRUNK_DR) %>% -->
<!--   select(-DAY_WEEK) %>% -->
<!--   unite(date, DAY:MINUTE, sep = "-") %>% -->
<!--   mutate(date = dmy_hm(date),  -->
<!--          drunk_dr = drunk_dr >= 1, -->
<!--          daytime = hour(date) %in% c(7:19), -->
<!--          month = month(date)) %>% -->
<!--   filter(!is.na(date)) -->

<!-- accident %>% slice(1:5) -->
<!-- ``` -->

<!-- A few notes: -->

<!-- - Notice that `select` is using the `:` operator to pick several columns in a row. -->
<!-- - Some of the column names in all caps are changed to lower case to make them easier to work with.  -->
<!-- - The `DAY_WEEK` column is in the middle of other date columns, but if you remove it, you can use `unite` with `:` to join together all the date-time columns and then use `lubridate` to change this column into the right class.  -->
<!-- - A logical operator is used inside a `mutate` call to create a column of whether the accident involved drunk driving (one or more drunk drivers involved) -->
<!-- - The `hour` function from `lubridate` is used to check if the time of the accident falls in "daytime" or not -->
<!-- - Some of the accidents are missing some date information. A `filter` is used to filter that out.  -->

<!-- Join the information from `state_fips` into the `accident` data frame. There may be a few locations in the `state_fips` data frame that are not included in the `accident` data frame (e.g., Virgin Islands), so when you join keep all observations in `accident` but only the observations in `state_fips` that match at least one row of `accident`. You can use the following code for this: -->

<!-- ```{r echo = FALSE} -->
<!-- accident <- accident %>% -->
<!--   left_join(state_fips, by = "state") -->

<!-- accident %>% slice(1:5) -->
<!-- ``` -->

<!-- Summarize the data to get the total number of accidents, separated by (1) daytime and nighttime and (2) related or unrelated to drunk driving, in each month (in other words, in January, how many daytime accidents were there that were unrelated to drunk driving? How many nighttime accidents that were unrelated to drunk driving? etc.). You can do that with this code: -->

<!-- ```{r} -->
<!-- accident %>% -->
<!--   filter(state_name == "Colorado") %>% -->
<!--   group_by(daytime, month, drunk_dr) %>% -->
<!--   summarize(accidents = n()) -->
<!-- ``` -->

<!-- As a note, you may want to create a table (for example, for a report) from the data at this stage. You could use `unite` then `pivot_wider` to do this pretty easily: -->

<!-- ```{r check5} -->
<!-- accident %>% -->
<!--   filter(state_name == "Colorado") %>% -->
<!--   mutate(daytime = factor(daytime, labels = c("Nighttime", "Daytime")), -->
<!--          drunk_dr = factor(drunk_dr,  -->
<!--                            labels = c("Not drunk driving", "Drunk driving"))) %>%  -->
<!--   group_by(daytime, month, drunk_dr) %>% -->
<!--   summarize(accidents = n()) %>%  -->
<!--   ungroup() %>%  -->
<!--   unite(category, daytime, drunk_dr, sep = " / ") %>%  -->
<!--   pivot_wider(names_from = category, values_from = accidents) %>%  -->
<!--   knitr::kable() -->
<!-- ``` -->

<!-- Write a function that inputs a data frame (`df`) and outputs this type of summary data frame (like the one just created for Colorado). You can do that with the following code. Note that, because it inputs a data frame and outputs a data frame, you can include it in a pipeline.  -->

<!-- ```{r} -->
<!-- summarize_fars <- function(df){ -->
<!--   df %>% -->
<!--     group_by(daytime, month, drunk_dr) %>% -->
<!--     summarize(accidents = n()) -->
<!-- } -->
<!-- ``` -->

<!-- Once you've written the function, see if you can figure out what the following code does. This code limits the data to data from four states and then applies the `summarize_fars` function that you just wrote to the subset of data from each state. Finally, since we `nest` to do that, the pipeline includes some lines to `unnest` the data to get back to an unnested data frame: -->

<!-- ```{r check4, eval = FALSE} -->
<!-- library(purrr) -->

<!-- accident %>%  -->
<!--   filter(state_name %in% c("Colorado", "Texas", "California", "New York")) %>%  -->
<!--   group_by(state_name) %>% -->
<!--   nest() %>%  -->
<!--   mutate(summary = map(data, summarize_fars)) %>%  -->
<!--   select(-data) %>%  -->
<!--   unnest()  -->
<!-- ``` -->

<!-- Write code to create boxplots for Colorado of the distribution of total accidents within each month. Create separate boxplots for daytime and nighttime accidents, and facet by whether the accident was related to drunk driving. You can do that with this code: -->

<!-- ```{r check3, fig.width = 6, fig.height = 3, fig.align = "center"} -->
<!-- accident %>% -->
<!--   filter(state_name == "Colorado") %>% -->
<!--   mutate(drunk_dr = factor(drunk_dr, labels = c("Unrelated to\ndrunk driving", -->
<!--                                                 "Related to\ndrunk driving")), -->
<!--          daytime = factor(daytime, labels = c("Nighttime", "Daytime"))) %>% -->
<!--   group_by(daytime, month, drunk_dr) %>% -->
<!--   summarize(accidents = n()) %>% -->
<!--   ggplot(aes(x = daytime, y = accidents, group = daytime)) +  -->
<!--   geom_boxplot() +  -->
<!--   facet_wrap(~ drunk_dr, ncol = 2) + -->
<!--   xlab("Time of day") + ylab("# of monthly accidents") +  -->
<!--   ggtitle(paste("Fatal accidents in", "Colorado", "in 2015")) -->
<!-- ``` -->


<!-- Now write a function called `plot_fars` to create a plot like the one you just made for Colorado for any data frame with the format of `accident` (i.e., same number, types, and names of columns). Test it on subsets of the data for several states (Colorado, Texas, California, and New York). (*Hint*: To get a function to print out a plot created with ggplot, you must explicitly print the plot object. For example, you could assign the plot to `fars_plot`, and then you would run `print(fars_plot)` within your loop as the last step.) -->

<!-- ```{r check2, echo = FALSE, fig.width = 6, fig.height = 3, fig.align = "center"} -->
<!-- plot_fars <- function(df){ -->
<!--   fars_plot <- df %>% -->
<!--     mutate(drunk_dr = factor(drunk_dr, labels = c("Unrelated to\ndrunk driving", -->
<!--                                                   "Related to\ndrunk driving")), -->
<!--            daytime = factor(daytime, labels = c("Nighttime", "Daytime"))) %>% -->
<!--     group_by(daytime, month, drunk_dr) %>% -->
<!--     summarize(accidents = n()) %>% -->
<!--     ggplot(aes(x = daytime, y = accidents, group = daytime)) + -->
<!--     geom_boxplot() + -->
<!--     facet_wrap(~ drunk_dr, ncol = 2) + -->
<!--     xlab("Time of day") + ylab("# of monthly accidents")  -->
<!--   print(fars_plot) -->
<!-- } -->
<!-- ``` -->

<!-- Notice how similar this function is to the code you wrote in the previous step.  -->

<!-- Once you have written this function, what happens when you run the following code? The following code applies this function to the subset of data from each of four states. The output (`state_plots`) is a nested data frame, where the new `plots` column is a list of `ggplot` objects. If you run `print` on this list, it will print each of these plots out separately (use the arrow buttons in the "Plots" Pane in RStudio to browse through these plots). -->

<!-- ```{r check1, fig.width = 6, fig.height = 3, fig.align = "center", eval = FALSE} -->
<!-- library(purrr) -->

<!-- state_plots <- accident %>%  -->
<!--   filter(state_name %in% c("Colorado", "Texas", "California", "New York")) %>%  -->
<!--   group_by(state_name) %>% -->
<!--   nest() %>%  -->
<!--   mutate(plots = map(data, plot_fars))  -->

<!-- state_plots -->
<!-- class(state_plots) -->
<!-- class(state_plots[["plots"]]) -->
<!-- class(state_plots[["plots"]][[1]]) -->
<!-- state_plots[["plots"]][[1]]$plot -->
<!-- ``` -->

<!-- Install the `cowplot` package (this is a `ggplot2` extension) and then try running the following code. What happens? The `plot_grid` function, if you input a list with `ggplot` objects using the `plotlist` argument, will print all the plots out on the same page.  -->

<!-- ```{r fig.width = 12, fig.height = 8, fig.align = "center", eval = FALSE} -->
<!-- library(cowplot) -->
<!-- plot_grid(plotlist = state_plots[["plots"]],  -->
<!--           ncol = 2, labels = "AUTO") -->
<!-- ``` -->

<!-- --- -->

